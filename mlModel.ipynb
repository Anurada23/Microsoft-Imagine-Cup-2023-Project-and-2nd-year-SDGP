{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modules loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4569/2680783035.py:26: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, Activation,Dropout,Conv2D, MaxPooling2D,BatchNormalization, Flatten\n",
    "from tensorflow.keras.optimizers import Adam, Adamax\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model, load_model, Sequential\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import time\n",
    "import cv2 as cv2\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from IPython.core.display import display, HTML\n",
    "# stop annoying tensorflow warning messages\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "print ('modules loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function provides a quick way to visualize a sample batch of images and their predicted classes generated by a data generator\n",
    "\n",
    "def show_image_samples(gen ):\n",
    "    t_dict=gen.class_indices\n",
    "    classes=list(t_dict.keys())    \n",
    "    images,labels=next(gen) # get a sample batch from the generator \n",
    "    plt.figure(figsize=(20, 20))\n",
    "    length=len(labels)\n",
    "    if length<25:   #show maximum of 25 images\n",
    "        r=length\n",
    "    else:\n",
    "        r=25\n",
    "    for i in range(r):\n",
    "        plt.subplot(5, 5, i + 1)\n",
    "        image=images[i]/255\n",
    "        plt.imshow(image)\n",
    "        index=np.argmax(labels[i])\n",
    "        class_name=classes[index]\n",
    "        plt.title(class_name, color='blue', fontsize=12)\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRA(keras.callbacks.Callback):\n",
    "    def __init__(self,model, base_model, patience,stop_patience, threshold, factor, dwell, batches, initial_epoch,epochs, ask_epoch):\n",
    "        super(LRA, self).__init__()\n",
    "        self.model=model\n",
    "        self.base_model=base_model\n",
    "        self.patience=patience # specifies how many epochs without improvement before learning rate is adjusted\n",
    "        self.stop_patience=stop_patience # specifies how many times to adjust lr without improvement to stop training\n",
    "        self.threshold=threshold # specifies training accuracy threshold when lr will be adjusted based on validation loss\n",
    "        self.factor=factor # factor by which to reduce the learning rate\n",
    "        self.dwell=dwell\n",
    "        self.batches=batches # number of training batch to runn per epoch\n",
    "        self.initial_epoch=initial_epoch\n",
    "        self.epochs=epochs\n",
    "        self.ask_epoch=ask_epoch\n",
    "        self.ask_epoch_initial=ask_epoch # save this value to restore if restarting training\n",
    "        # callback variables \n",
    "        self.count=0 # how many times lr has been reduced without improvement\n",
    "        self.stop_count=0        \n",
    "        self.best_epoch=1   # epoch with the lowest loss        \n",
    "        self.initial_lr=float(tf.keras.backend.get_value(model.optimizer.lr)) # get the initiallearning rate and save it         \n",
    "        self.highest_tracc=0.0 # set highest training accuracy to 0 initially\n",
    "        self.lowest_vloss=np.inf # set lowest validation loss to infinity initially\n",
    "        self.best_weights=self.model.get_weights() # set best weights to model's initial weights\n",
    "        self.initial_weights=self.model.get_weights()   # save initial weights if they have to get restored \n",
    "        \n",
    "    def on_train_begin(self, logs=None):        \n",
    "        if self.base_model != None:\n",
    "            status=base_model.trainable\n",
    "            if status:\n",
    "                msg=' initializing callback starting training with base_model trainable'\n",
    "            else:\n",
    "                msg='initializing callback starting training with base_model not trainable'\n",
    "        else:\n",
    "            msg='initialing callback and starting training'                        \n",
    "            print(msg)\n",
    "        msg='{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:^9s}{7:^10s}{8:10s}{9:^8s}'.format('Epoch', 'Loss', 'Accuracy','V_loss','V_acc', 'LR', 'Next LR', 'Monitor','% Improv', 'Duration')\n",
    "        print(msg)\n",
    "        self.start_time= time.time()\n",
    "        \n",
    "    def on_train_end(self, logs=None):\n",
    "        stop_time=time.time()\n",
    "        tr_duration= stop_time- self.start_time            \n",
    "        hours = tr_duration // 3600\n",
    "        minutes = (tr_duration - (hours * 3600)) // 60\n",
    "        seconds = tr_duration - ((hours * 3600) + (minutes * 60))\n",
    "\n",
    "        self.model.set_weights(self.best_weights) # set the weights of the model to the best weights\n",
    "        msg=f'Training is completed - model is set with weights from epoch {self.best_epoch} '\n",
    "        print(msg)      \n",
    "        msg = f'training elapsed time was {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'\n",
    "        print(msg)\n",
    "        \n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        acc=logs.get('accuracy')* 100  # get training accuracy \n",
    "        loss=logs.get('loss')\n",
    "        msg='{0:20s}processing batch {1:4s} of {2:5s} accuracy= {3:8.3f}  loss: {4:8.5f}'.format(' ', str(batch), str(self.batches), acc, loss)\n",
    "        print(msg, '\\r', end='') # prints over on the same line to show running batch count        \n",
    "        \n",
    "    def on_epoch_begin(self,epoch, logs=None):\n",
    "        self.now= time.time()\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):  # method runs on the end of each epoch\n",
    "        later=time.time()\n",
    "        duration=later-self.now \n",
    "        lr=float(tf.keras.backend.get_value(self.model.optimizer.lr)) # get the current learning rate\n",
    "        current_lr=lr\n",
    "        v_loss=logs.get('val_loss')  # get the validation loss for this epoch\n",
    "        acc=logs.get('accuracy')  # get training accuracy \n",
    "        v_acc=logs.get('val_accuracy')\n",
    "        loss=logs.get('loss')        \n",
    "        if acc < self.threshold: # if training accuracy is below threshold adjust lr based on training accuracy\n",
    "            monitor='accuracy'\n",
    "            if epoch ==0:\n",
    "                pimprov=0.0\n",
    "            else:\n",
    "                pimprov= (acc-self.highest_tracc )*100/self.highest_tracc\n",
    "            if acc>self.highest_tracc: # training accuracy improved in the epoch                \n",
    "                self.highest_tracc=acc # set new highest training accuracy\n",
    "                self.best_weights=self.model.get_weights() # traing accuracy improved so save the weights\n",
    "                self.count=0 # set count to 0 since training accuracy improved\n",
    "                self.stop_count=0 # set stop counter to 0\n",
    "                if v_loss<self.lowest_vloss:\n",
    "                    self.lowest_vloss=v_loss\n",
    "                color= (0,255,0)\n",
    "                self.best_epoch=epoch + 1  # set the value of best epoch for this epoch              \n",
    "            else: \n",
    "                # training accuracy did not improve check if this has happened for patience number of epochs\n",
    "                # if so adjust learning rate\n",
    "                if self.count>=self.patience -1: # lr should be adjusted\n",
    "                    color=(245, 170, 66)\n",
    "                    lr= lr* self.factor # adjust the learning by factor\n",
    "                    tf.keras.backend.set_value(self.model.optimizer.lr, lr) # set the learning rate in the optimizer\n",
    "                    self.count=0 # reset the count to 0\n",
    "                    self.stop_count=self.stop_count + 1 # count the number of consecutive lr adjustments\n",
    "                    self.count=0 # reset counter\n",
    "                    if self.dwell:\n",
    "                        self.model.set_weights(self.best_weights) # return to better point in N space                        \n",
    "                    else:\n",
    "                        if v_loss<self.lowest_vloss:\n",
    "                            self.lowest_vloss=v_loss                                    \n",
    "                else:\n",
    "                    self.count=self.count +1 # increment patience counter                    \n",
    "        else: # training accuracy is above threshold so adjust learning rate based on validation loss\n",
    "            monitor='val_loss'\n",
    "            if epoch ==0:\n",
    "                pimprov=0.0\n",
    "            else:\n",
    "                pimprov= (self.lowest_vloss- v_loss )*100/self.lowest_vloss\n",
    "            if v_loss< self.lowest_vloss: # check if the validation loss improved \n",
    "                self.lowest_vloss=v_loss # replace lowest validation loss with new validation loss                \n",
    "                self.best_weights=self.model.get_weights() # validation loss improved so save the weights\n",
    "                self.count=0 # reset count since validation loss improved  \n",
    "                self.stop_count=0  \n",
    "                color=(0,255,0)                \n",
    "                self.best_epoch=epoch + 1 # set the value of the best epoch to this epoch\n",
    "            else: # validation loss did not improve\n",
    "                if self.count>=self.patience-1: # need to adjust lr\n",
    "                    color=(245, 170, 66)\n",
    "                    lr=lr * self.factor # adjust the learning rate                    \n",
    "                    self.stop_count=self.stop_count + 1 # increment stop counter because lr was adjusted \n",
    "                    self.count=0 # reset counter\n",
    "                    tf.keras.backend.set_value(self.model.optimizer.lr, lr) # set the learning rate in the optimizer\n",
    "                    if self.dwell:\n",
    "                        self.model.set_weights(self.best_weights) # return to better point in N space\n",
    "                else: \n",
    "                    self.count =self.count +1 # increment the patience counter                    \n",
    "                if acc>self.highest_tracc:\n",
    "                    self.highest_tracc= acc\n",
    "        msg=f'{str(epoch+1):^3s}/{str(self.epochs):4s} {loss:^9.3f}{acc*100:^9.3f}{v_loss:^9.5f}{v_acc*100:^9.3f}{current_lr:^9.5f}{lr:^9.5f}{monitor:^11s}{pimprov:^10.2f}{duration:^8.2f}'\n",
    "        print(msg)\n",
    "        if self.stop_count> self.stop_patience - 1: # check if learning rate has been adjusted stop_count times with no improvement\n",
    "            msg=f' training has been halted at epoch {epoch + 1} after {self.stop_patience} adjustments of learning rate with no improvement'\n",
    "            print(msg)            \n",
    "            self.model.stop_training = True # stop training\n",
    "        else: \n",
    "            if self.ask_epoch !=None:\n",
    "                if epoch + 1 >= self.ask_epoch:\n",
    "                    if base_model.trainable:\n",
    "                        msg='enter H to halt training or an integer for number of epochs to run then ask again'\n",
    "                    else:\n",
    "                        msg='enter H to halt training ,F to fine tune model, or an integer for number of epochs to run then ask again'\n",
    "                    print(msg)                    \n",
    "                    ans=input('')\n",
    "                    if ans=='H' or ans=='h':\n",
    "                        msg=f'training has been halted at epoch {epoch + 1} due to user input'\n",
    "                        print(msg)               \n",
    "                        self.model.stop_training = True # stop training\n",
    "                    elif ans == 'F' or ans=='f':\n",
    "                        if base_model.trainable:\n",
    "                            msg='base_model is already set as trainable'\n",
    "                        else:\n",
    "                            msg='setting base_model as trainable for fine tuning of model'\n",
    "                            self.base_model.trainable=True\n",
    "                        print(msg)\n",
    "                        msg='{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:^9s}{7:^10s}{8:^8s}'.format('Epoch', 'Loss', 'Accuracy',\n",
    "                                                                                              'V_loss','V_acc', 'LR', 'Next LR', 'Monitor','% Improv', 'Duration')\n",
    "                        print(msg)                        \n",
    "                        self.count=0\n",
    "                        self.stop_count=0                        \n",
    "                        self.ask_epoch = epoch + 1 + self.ask_epoch_initial \n",
    "                        \n",
    "                    else:\n",
    "                        ans=int(ans)\n",
    "                        self.ask_epoch +=ans\n",
    "                        msg=f' training will continue until epoch ' + str(self.ask_epoch)                         \n",
    "                        print(msg)\n",
    "                        msg='{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:^9s}{7:^10s}{8:10s}{9:^8s}'.format('Epoch', 'Loss', 'Accuracy',\n",
    "                                                                                              'V_loss','V_acc', 'LR', 'Next LR', 'Monitor','% Improv', 'Duration')\n",
    "                        print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#showing training and validation Losse\n",
    "\n",
    "def tr_plot(tr_data, start_epoch):\n",
    "    #Plot the training and validation data\n",
    "    tacc=tr_data.history['accuracy']\n",
    "    tloss=tr_data.history['loss']\n",
    "    vacc=tr_data.history['val_accuracy']\n",
    "    vloss=tr_data.history['val_loss']\n",
    "    Epoch_count=len(tacc)+ start_epoch\n",
    "    Epochs=[]\n",
    "    for i in range (start_epoch ,Epoch_count):\n",
    "        Epochs.append(i+1)   \n",
    "    index_loss=np.argmin(vloss)#  this is the epoch with the lowest validation loss\n",
    "    val_lowest=vloss[index_loss]\n",
    "    index_acc=np.argmax(vacc)\n",
    "    acc_highest=vacc[index_acc]\n",
    "    plt.style.use('fivethirtyeight')\n",
    "    sc_label='best epoch= '+ str(index_loss+1 +start_epoch)\n",
    "    vc_label='best epoch= '+ str(index_acc + 1+ start_epoch)\n",
    "    fig,axes=plt.subplots(nrows=1, ncols=2, figsize=(20,8))\n",
    "    axes[0].plot(Epochs,tloss, 'r', label='Training loss')\n",
    "    axes[0].plot(Epochs,vloss,'g',label='Validation loss' )\n",
    "    axes[0].scatter(index_loss+1 +start_epoch,val_lowest, s=150, c= 'blue', label=sc_label)\n",
    "    axes[0].set_title('Training and Validation Loss')\n",
    "    axes[0].set_xlabel('Epochs')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend()\n",
    "    axes[1].plot (Epochs,tacc,'r',label= 'Training Accuracy')\n",
    "    axes[1].plot (Epochs,vacc,'g',label= 'Validation Accuracy')\n",
    "    axes[1].scatter(index_acc+1 +start_epoch,acc_highest, s=150, c= 'blue', label=vc_label)\n",
    "    axes[1].set_title('Training and Validation Accuracy')\n",
    "    axes[1].set_xlabel('Epochs')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].legend()\n",
    "    plt.tight_layout\n",
    "    #plt.style.use('fivethirtyeight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The function then generates a confusion matrix and classification report based on the true labels and predicted classes.\n",
    "\n",
    "def print_info( test_gen, preds, print_code, save_dir, subject ):\n",
    "    class_dict=test_gen.class_indices\n",
    "    labels= test_gen.labels\n",
    "    file_names= test_gen.filenames \n",
    "    error_list=[]\n",
    "    true_class=[]\n",
    "    pred_class=[]\n",
    "    prob_list=[]\n",
    "    new_dict={}\n",
    "    error_indices=[]\n",
    "    y_pred=[]\n",
    "    for key,value in class_dict.items():\n",
    "        new_dict[value]=key             # dictionary {integer of class number: string of class name}\n",
    "    # store new_dict as a text fine in the save_dir\n",
    "    classes=list(new_dict.values())     # list of string of class names     \n",
    "    errors=0      \n",
    "    for i, p in enumerate(preds):\n",
    "        pred_index=np.argmax(p)         \n",
    "        true_index=labels[i]  # labels are integer values\n",
    "        if pred_index != true_index: # a misclassification has occurred\n",
    "            error_list.append(file_names[i])\n",
    "            true_class.append(new_dict[true_index])\n",
    "            pred_class.append(new_dict[pred_index])\n",
    "            prob_list.append(p[pred_index])\n",
    "            error_indices.append(true_index)            \n",
    "            errors=errors + 1\n",
    "        y_pred.append(pred_index)    \n",
    "    if print_code !=0:\n",
    "        if errors>0:\n",
    "            if print_code>errors:\n",
    "                r=errors\n",
    "            else:\n",
    "                r=print_code           \n",
    "            msg='{0:^28s}{1:^28s}{2:^28s}{3:^16s}'.format('Filename', 'Predicted Class' , 'True Class', 'Probability')\n",
    "            print(msg)\n",
    "            for i in range(r):                \n",
    "                split1=os.path.split(error_list[i])                \n",
    "                split2=os.path.split(split1[0])                \n",
    "                fname=split2[1] + '/' + split1[1]\n",
    "                msg='{0:^28s}{1:^28s}{2:^28s}{3:4s}{4:^6.4f}'.format(fname, pred_class[i],true_class[i], ' ', prob_list[i])\n",
    "                print(msg)\n",
    "\n",
    "                #print(error_list[i]  , pred_class[i], true_class[i], prob_list[i])               \n",
    "        else:\n",
    "            msg='With accuracy of 100 % there are no errors to print'\n",
    "            print(msg)\n",
    "        if errors>0:\n",
    "            plot_bar=[]\n",
    "            plot_class=[]\n",
    "        for  key, value in new_dict.items():        \n",
    "            count=error_indices.count(key) \n",
    "            if count!=0:\n",
    "                plot_bar.append(count) # list containg how many times a class c had an error\n",
    "                plot_class.append(value)   # stores the class \n",
    "        fig=plt.figure()\n",
    "        fig.set_figheight(len(plot_class)/3)\n",
    "        fig.set_figwidth(10)\n",
    "        plt.style.use('fivethirtyeight')\n",
    "        for i in range(0, len(plot_class)):\n",
    "            c=plot_class[i]\n",
    "            x=plot_bar[i]\n",
    "            plt.barh(c, x, )\n",
    "            plt.title( ' Errors by Class on Test Set')\n",
    "    y_true= np.array(labels)        \n",
    "    y_pred=np.array(y_pred)\n",
    "    if len(classes)<= 30:\n",
    "        # create a confusion matrix \n",
    "        cm = confusion_matrix(y_true, y_pred )        \n",
    "        length=len(classes)\n",
    "        if length<8:\n",
    "            fig_width=8\n",
    "            fig_height=8\n",
    "        else:\n",
    "            fig_width= int(length * .5)\n",
    "            fig_height= int(length * .5)\n",
    "        plt.figure(figsize=(fig_width, fig_height))\n",
    "        sns.heatmap(cm, annot=True, vmin=0, fmt='g', cmap='Blues', cbar=False)       \n",
    "        plt.xticks(np.arange(length)+.5, classes, rotation= 90)\n",
    "        plt.yticks(np.arange(length)+.5, classes, rotation=0)\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.show()\n",
    "    clr = classification_report(y_true, y_pred, target_names=classes)\n",
    "    print(\"Classification Report:\\n----------------------\\n\", clr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function saving trained model\n",
    "def saver(save_path, model, model_name, subject, accuracy,img_size, scalar, generator):\n",
    "    print ('the save path is: ', save_path)\n",
    "    # first save the model\n",
    "    save_id=str (model_name +  '-' + subject +'-'+ str(acc)[:str(acc).rfind('.')+3] + '.h5')\n",
    "    model_save_loc=os.path.join(save_path, save_id)\n",
    "    model.save(model_save_loc)\n",
    "    print ('model was saved as ' + model_save_loc) \n",
    "    # now create the class_df and convert to csv file    \n",
    "    class_dict=generator.class_indices \n",
    "    height=[]\n",
    "    width=[]\n",
    "    scale=[]\n",
    "    for i in range(len(class_dict)):\n",
    "        height.append(img_size[0])\n",
    "        width.append(img_size[1])\n",
    "        scale.append(scalar)\n",
    "    Index_series=pd.Series(list(class_dict.values()), name='class_index')\n",
    "    Class_series=pd.Series(list(class_dict.keys()), name='class') \n",
    "    Height_series=pd.Series(height, name='height')\n",
    "    Width_series=pd.Series(width, name='width')\n",
    "    Scale_series=pd.Series(scale, name='scale by')\n",
    "    class_df=pd.concat([Index_series, Class_series, Height_series, Width_series, Scale_series], axis=1)    \n",
    "    csv_name='class_dict.csv'\n",
    "    csv_save_loc=os.path.join(save_path, csv_name)\n",
    "    class_df.to_csv(csv_save_loc, index=False) \n",
    "    print ('class csv file was saved as ' + csv_save_loc) \n",
    "    return model_save_loc, csv_save_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(sdir, csv_path,  model_path, averaged=True, verbose=True):    \n",
    "    # read in the csv file\n",
    "    class_df=pd.read_csv(csv_path)    \n",
    "    class_count=len(class_df['class'].unique())\n",
    "    img_height=int(class_df['height'].iloc[0])\n",
    "    img_width =int(class_df['width'].iloc[0])\n",
    "    img_size=(img_width, img_height)    \n",
    "    scale=class_df['scale by'].iloc[0] \n",
    "    image_list=[]\n",
    "    # determine value to scale image pixels by\n",
    "    try: \n",
    "        s=int(scale)\n",
    "        s2=1\n",
    "        s1=0\n",
    "    except:\n",
    "        split=scale.split('-')\n",
    "        s1=float(split[1])\n",
    "        s2=float(split[0].split('*')[1])\n",
    "    path_list=[]\n",
    "    paths=os.listdir(sdir)    \n",
    "    for f in paths:\n",
    "        path_list.append(os.path.join(sdir,f))\n",
    "    if verbose:\n",
    "        print (' Model is being loaded- this will take about 10 seconds')\n",
    "    model=load_model(model_path)\n",
    "    image_count=len(path_list) \n",
    "    image_list=[]\n",
    "    file_list=[]\n",
    "    good_image_count=0\n",
    "    for i in range (image_count):        \n",
    "        try:\n",
    "            img=cv2.imread(path_list[i])\n",
    "            img=cv2.resize(img, img_size)\n",
    "            img=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)            \n",
    "            good_image_count +=1\n",
    "            img=img*s2 - s1             \n",
    "            image_list.append(img)\n",
    "            file_name=os.path.split(path_list[i])[1]\n",
    "            file_list.append(file_name)\n",
    "        except:\n",
    "            if verbose:\n",
    "                print ( path_list[i], ' is an invalid image file')\n",
    "    if good_image_count==1: # if only a single image need to expand dimensions\n",
    "        averaged=True\n",
    "    image_array=np.array(image_list)    \n",
    "    # make predictions on images, sum the probabilities of each class then find class index with\n",
    "    # highest probability\n",
    "    preds=model.predict(image_array)    \n",
    "    if averaged:\n",
    "        psum=[]\n",
    "        for i in range (class_count): # create all 0 values list\n",
    "            psum.append(0)    \n",
    "        for p in preds: # iterate over all predictions\n",
    "            for i in range (class_count):\n",
    "                psum[i]=psum[i] + p[i]  # sum the probabilities   \n",
    "        index=np.argmax(psum) # find the class index with the highest probability sum        \n",
    "        klass=class_df['class'].iloc[index] # get the class name that corresponds to the index\n",
    "        prob=psum[index]/good_image_count  # get the probability average         \n",
    "        # to show the correct image run predict again and select first image that has same index\n",
    "        for img in image_array:  #iterate through the images    \n",
    "            test_img=np.expand_dims(img, axis=0) # since it is a single image expand dimensions \n",
    "            test_index=np.argmax(model.predict(test_img)) # for this image find the class index with highest probability\n",
    "            if test_index== index: # see if this image has the same index as was selected previously\n",
    "                if verbose: # show image and print result if verbose=1\n",
    "                    plt.axis('off')\n",
    "                    plt.imshow(img) # show the image\n",
    "                    print (f'predicted species is {klass} with a probability of {prob:6.4f} ')\n",
    "                break # found an image that represents the predicted class      \n",
    "        return klass, prob, img, None\n",
    "    else: # create individual predictions for each image\n",
    "        pred_class=[]\n",
    "        prob_list=[]\n",
    "        for i, p in enumerate(preds):\n",
    "            index=np.argmax(p) # find the class index with the highest probability sum\n",
    "            klass=class_df['class'].iloc[index] # get the class name that corresponds to the index\n",
    "            image_file= file_list[i]\n",
    "            pred_class.append(klass)\n",
    "            prob_list.append(p[index])            \n",
    "        Fseries=pd.Series(file_list, name='image file')\n",
    "        Lseries=pd.Series(pred_class, name= 'species')\n",
    "        Pseries=pd.Series(prob_list, name='probability')\n",
    "        df=pd.concat([Fseries, Lseries, Pseries], axis=1)\n",
    "        if verbose:\n",
    "            length= len(df)\n",
    "            print (df.head(length))\n",
    "        return None, None, None, df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trim a pandas DataFrame so that each group in a specified column has no more than max_size samples and at least min_size samples.\n",
    "def trim (df, max_size, min_size, column):\n",
    "    df=df.copy()\n",
    "    original_class_count= len(list(df[column].unique()))\n",
    "    print ('Original Number of classes in dataframe: ', original_class_count)\n",
    "    sample_list=[] \n",
    "    groups=df.groupby(column)\n",
    "    for label in df[column].unique():        \n",
    "        group=groups.get_group(label)\n",
    "        sample_count=len(group)         \n",
    "        if sample_count> max_size :\n",
    "            strat=group[column]\n",
    "            samples,_=train_test_split(group, train_size=max_size, shuffle=True, random_state=123, stratify=strat)            \n",
    "            sample_list.append(samples)\n",
    "        elif sample_count>= min_size:\n",
    "            sample_list.append(group)\n",
    "    df=pd.concat(sample_list, axis=0).reset_index(drop=True)\n",
    "    final_class_count= len(list(df[column].unique())) \n",
    "    if final_class_count != original_class_count:\n",
    "        print ('*** WARNING***  dataframe has a reduced number of classes' )\n",
    "    balance=list(df[column].value_counts())\n",
    "    print (balance)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function balances the number of samples in each class of a training dataset by creating\n",
    "def balance(train_df,max_samples, min_samples, column, working_dir, image_size):\n",
    "    train_df=train_df.copy()\n",
    "    train_df=trim (train_df, max_samples, min_samples, column)    \n",
    "    # make directories to store augmented images\n",
    "    aug_dir=os.path.join(working_dir, 'aug')\n",
    "    if os.path.isdir(aug_dir):\n",
    "        shutil.rmtree(aug_dir)\n",
    "    os.mkdir(aug_dir)\n",
    "    for label in train_df['labels'].unique():    \n",
    "        dir_path=os.path.join(aug_dir,label)    \n",
    "        os.mkdir(dir_path)\n",
    "    # create and store the augmented images  \n",
    "    total=0\n",
    "    gen=ImageDataGenerator(horizontal_flip=True,  rotation_range=20, width_shift_range=.2,\n",
    "                                  height_shift_range=.2, zoom_range=.2)\n",
    "    groups=train_df.groupby('labels') # group by class\n",
    "    for label in train_df['labels'].unique():  # for every class               \n",
    "        group=groups.get_group(label)  # a dataframe holding only rows with the specified label \n",
    "        sample_count=len(group)   # determine how many samples there are in this class  \n",
    "        if sample_count< max_samples: # if the class has less than target number of images\n",
    "            aug_img_count=0\n",
    "            delta=max_samples-sample_count  # number of augmented images to create\n",
    "            target_dir=os.path.join(aug_dir, label)  # define where to write the images    \n",
    "            aug_gen=gen.flow_from_dataframe( group,  x_col='filepaths', y_col=None, target_size=image_size,\n",
    "                                            class_mode=None, batch_size=1, shuffle=False, \n",
    "                                            save_to_dir=target_dir, save_prefix='aug-', color_mode='rgb',\n",
    "                                            save_format='jpg')\n",
    "            while aug_img_count<delta:\n",
    "                images=next(aug_gen)            \n",
    "                aug_img_count += len(images)\n",
    "            total +=aug_img_count\n",
    "    print('Total Augmented images created= ', total)\n",
    "    # create aug_df and merge with train_df to create composite training set ndf\n",
    "    if total>0:\n",
    "        aug_fpaths=[]\n",
    "        aug_labels=[]\n",
    "        classlist=os.listdir(aug_dir)\n",
    "        for klass in classlist:\n",
    "            classpath=os.path.join(aug_dir, klass)     \n",
    "            flist=os.listdir(classpath)    \n",
    "            for f in flist:        \n",
    "                fpath=os.path.join(classpath,f)         \n",
    "                aug_fpaths.append(fpath)\n",
    "                aug_labels.append(klass)\n",
    "        Fseries=pd.Series(aug_fpaths, name='filepaths')\n",
    "        Lseries=pd.Series(aug_labels, name='labels')\n",
    "        aug_df=pd.concat([Fseries, Lseries], axis=1)\n",
    "        train_df=pd.concat([train_df,aug_df], axis=0).reset_index(drop=True)\n",
    "   \n",
    "    print (list(train_df['labels'].value_counts()) )\n",
    "    return train_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51, 46, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f016011ba00>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#check image showing is correct\n",
    "img_path='../project/archive/clubbing/3 (1).jpg'\n",
    "img=plt.imread(img_path)\n",
    "print (img.shape)\n",
    "imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Given a source directory containing subdirectories of images, this function preprocesses the images\n",
    "#     and returns three dataframes for training, validation, and testing sets.\n",
    "def preprocess (sdir, trsplit, vsplit):\n",
    "    filepaths=[]\n",
    "    labels=[]\n",
    "    classlist=os.listdir(sdir)\n",
    "    for klass in classlist:\n",
    "        classpath=os.path.join(sdir,klass)\n",
    "        flist=os.listdir(classpath)\n",
    "        for f in flist:\n",
    "            fpath=os.path.join(classpath, f)\n",
    "            filepaths.append(fpath)\n",
    "            labels.append(klass)\n",
    "    Fseries=pd.Series(filepaths, name='filepaths')\n",
    "    Lseries=pd.Series(labels, name='labels')\n",
    "    df=pd.concat([Fseries, Lseries], axis=1)            \n",
    "    dsplit=vsplit/(1-trsplit)\n",
    "    strat=df['labels']\n",
    "    train_df, dummy_df=train_test_split(df, train_size=trsplit, shuffle=True, random_state=123, stratify=strat)\n",
    "    strat=dummy_df['labels']\n",
    "    valid_df, test_df= train_test_split(dummy_df, train_size=dsplit, shuffle=True, random_state=123, stratify=strat)\n",
    "    print('train_df length: ', len(train_df), '  test_df length: ',len(test_df), '  valid_df length: ', len(valid_df))\n",
    "     # check that each dataframe has the same number of classes to prevent model.fit errors\n",
    "    trcount=len(train_df['labels'].unique())\n",
    "    tecount=len(test_df['labels'].unique())\n",
    "    vcount=len(valid_df['labels'].unique())\n",
    "    if trcount < tecount :         \n",
    "        msg='** WARNING ** number of classes in training set is less than the number of classes in test set'\n",
    "        print(msg)\n",
    "        msg='This will throw an error in either model.evaluate or model.predict'\n",
    "        print(msg)\n",
    "    if trcount != vcount:\n",
    "        msg='** WARNING ** number of classes in training set not equal to number of classes in validation set' \n",
    "        print(msg)\n",
    "        msg=' this will throw an error in model.fit'\n",
    "        print(msg)\n",
    "        print ('train df class count: ', trcount, 'test df class count: ', tecount, ' valid df class count: ', vcount) \n",
    "        ans=input('Enter C to continue execution or H to halt execution')\n",
    "        if ans =='H' or ans == 'h':\n",
    "            print('Halting Execution')\n",
    "            import sys\n",
    "            sys.exit('program halted by user')            \n",
    "    print(list(train_df['labels'].value_counts()))\n",
    "    return train_df, test_df, valid_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df length:  560   test_df length:  70   valid_df length:  70\n",
      "[80, 80, 80, 80, 80, 80, 80]\n"
     ]
    }
   ],
   "source": [
    "sdir=r'../project/archive'\n",
    "trsplit=.8\n",
    "vsplit=.1\n",
    "train_df, test_df, valid_df= preprocess(sdir,trsplit, vsplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Number of classes in dataframe:  7\n",
      "[80, 80, 80, 80, 80, 80, 80]\n",
      "Found 80 validated image filenames.\n",
      "Found 80 validated image filenames.\n",
      "Found 80 validated image filenames.\n",
      "Found 80 validated image filenames.\n",
      "Found 80 validated image filenames.\n",
      "Found 80 validated image filenames.\n",
      "Found 80 validated image filenames.\n",
      "Total Augmented images created=  490\n",
      "[150, 150, 150, 150, 150, 150, 150]\n"
     ]
    }
   ],
   "source": [
    "max_samples= 150\n",
    "min_samples=0\n",
    "column='labels'\n",
    "working_dir = r'./'\n",
    "img_size=(224,224)\n",
    "train_df=balance(train_df, max_samples, min_samples, column, working_dir, img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test batch size:  70   test steps:  1\n",
      "Found 1050 validated image filenames belonging to 7 classes.\n",
      "Found 70 validated image filenames belonging to 7 classes.\n",
      "Found 70 validated image filenames belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "channels=3\n",
    "batch_size=40\n",
    "img_shape=(img_size[0], img_size[1], channels)\n",
    "length=len(test_df)\n",
    "test_batch_size=sorted([int(length/n) for n in range(1,length+1) if length % n ==0 and length/n<=80],reverse=True)[0]  \n",
    "test_steps=int(length/test_batch_size)\n",
    "print ( 'test batch size: ' ,test_batch_size, '  test steps: ', test_steps)\n",
    "def scalar(img):    \n",
    "    return img  # EfficientNet expects pixelsin range 0 to 255 so no scaling is required\n",
    "trgen=ImageDataGenerator(preprocessing_function=scalar, horizontal_flip=True)\n",
    "tvgen=ImageDataGenerator(preprocessing_function=scalar)\n",
    "train_gen=trgen.flow_from_dataframe( train_df, x_col='filepaths', y_col='labels', target_size=img_size, class_mode='categorical',\n",
    "                                    color_mode='rgb', shuffle=True, batch_size=batch_size)\n",
    "test_gen=tvgen.flow_from_dataframe( test_df, x_col='filepaths', y_col='labels', target_size=img_size, class_mode='categorical',\n",
    "                                    color_mode='rgb', shuffle=False, batch_size=test_batch_size)\n",
    "\n",
    "valid_gen=tvgen.flow_from_dataframe( valid_df, x_col='filepaths', y_col='labels', target_size=img_size, class_mode='categorical',\n",
    "                                    color_mode='rgb', shuffle=True, batch_size=batch_size)\n",
    "classes=list(train_gen.class_indices.keys())\n",
    "class_count=len(classes)\n",
    "train_steps=int(np.ceil(len(train_gen.labels)/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'show_image_samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m show_image_samples(train_gen)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'show_image_samples' is not defined"
     ]
    }
   ],
   "source": [
    "#test image samples\n",
    "show_image_samples(train_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb3_notop.h5\n",
      "43941136/43941136 [==============================] - 126s 3us/step\n"
     ]
    }
   ],
   "source": [
    "model_name='EfficientNetB3'\n",
    "base_model=tf.keras.applications.EfficientNetB3(include_top=False, weights=\"imagenet\",input_shape=img_shape, pooling='max') \n",
    "x=base_model.output\n",
    "x=keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(x)\n",
    "x = Dense(256, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),\n",
    "                bias_regularizer=regularizers.l1(0.006) ,activation='relu')(x)\n",
    "x=Dropout(rate=.45, seed=123)(x)        \n",
    "output=Dense(class_count, activation='softmax')(x)\n",
    "model=Model(inputs=base_model.input, outputs=output)\n",
    "model.compile(Adamax(learning_rate=.001), loss='categorical_crossentropy', metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs =40\n",
    "patience= 1 # number of epochs to wait to adjust lr if monitored value does not improve\n",
    "stop_patience =3 # number of epochs to wait before stopping training if monitored value does not improve\n",
    "threshold=.9 # if train accuracy is < threshhold adjust monitor accuracy, else monitor validation loss\n",
    "factor=.5 # factor to reduce lr by\n",
    "dwell=True # experimental, if True and monitored metric does not improve on current epoch set  modelweights back to weights of previous epoch\n",
    "freeze=False # if true free weights of  the base model\n",
    "ask_epoch=15# number of epochs to run before asking if you want to halt training\n",
    "batches=train_steps\n",
    "callbacks=[LRA(model=model,base_model= base_model,patience=patience,stop_patience=stop_patience, threshold=threshold,\n",
    "                   factor=factor,dwell=dwell, batches=batches,initial_epoch=0,epochs=epochs, ask_epoch=ask_epoch )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;244;252;3;48;2;55;65;80m initializing callback starting training with base_model trainable\n",
      "\u001b[0m\n",
      "\u001b[38;2;244;252;3;48;2;55;65;80m Epoch     Loss   Accuracy  V_loss    V_acc     LR     Next LR  Monitor  % Improv  Duration\n",
      "\u001b[0m\n",
      "\u001b[38;2;0;255;0;48;2;55;65;80m 1 /40     9.742   33.619   9.59075  48.571   0.00100  0.00100  accuracy     0.00    636.84 \n",
      "\u001b[0m\n",
      "\u001b[38;2;0;255;0;48;2;55;65;80m 2 /40     8.036   70.000   8.18577  58.571   0.00100  0.00100  accuracy    108.22   603.35 \n",
      "\u001b[0m\n",
      "                    processing batch 20   of 27    accuracy=   86.190  loss:  7.21231 \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39;49mfit(x\u001b[39m=\u001b[39;49mtrain_gen,  epochs\u001b[39m=\u001b[39;49mepochs, verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49mcallbacks,  validation_data\u001b[39m=\u001b[39;49mvalid_gen,\n\u001b[1;32m      2\u001b[0m                validation_steps\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,  shuffle\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,  initial_epoch\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1651\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history=model.fit(x=train_gen,  epochs=epochs, verbose=0, callbacks=callbacks,  validation_data=valid_gen,\n",
    "               validation_steps=None,  shuffle=False,  initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tr_plot(history,\u001b[39m0\u001b[39m)\n\u001b[1;32m      2\u001b[0m subject\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnails\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      3\u001b[0m acc\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mevaluate( test_gen, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, steps\u001b[39m=\u001b[39mtest_steps, return_dict\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)[\u001b[39m1\u001b[39m]\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "tr_plot(history,0)\n",
    "subject='nails'\n",
    "acc=model.evaluate( test_gen, verbose=1, steps=test_steps, return_dict=False)[1]*100\n",
    "msg=f'accuracy on the test set is {acc:5.2f} %'\n",
    "print(msg)\n",
    "generator=train_gen\n",
    "scale = 1\n",
    "model_save_loc, csv_save_loc=saver(working_dir, model, model_name, subject, acc, img_size, scale,  generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 12s 12s/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'subject' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m print_code\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m      2\u001b[0m preds\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mpredict(test_gen, steps\u001b[39m=\u001b[39mtest_steps, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \n\u001b[0;32m----> 3\u001b[0m print_info( test_gen, preds, print_code, working_dir, subject )  \n",
      "\u001b[0;31mNameError\u001b[0m: name 'subject' is not defined"
     ]
    }
   ],
   "source": [
    "print_code=0\n",
    "preds=model.predict(test_gen, steps=test_steps, verbose=1) \n",
    "print_info( test_gen, preds, print_code, working_dir, subject )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input image shape is  (75, 65, 4)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'img_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m (\u001b[39m'\u001b[39m\u001b[39mInput image shape is \u001b[39m\u001b[39m'\u001b[39m, img\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      4\u001b[0m \u001b[39m# resize the image so it is the same size as the images the model was trained on\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m img\u001b[39m=\u001b[39mcv2\u001b[39m.\u001b[39mresize(img, img_size) \u001b[39m# in earlier code img_size=(224,224) was used for training the model\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m (\u001b[39m'\u001b[39m\u001b[39mthe resized image has shape \u001b[39m\u001b[39m'\u001b[39m, img\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      7\u001b[0m \u001b[39m### show the resized image\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'img_size' is not defined"
     ]
    }
   ],
   "source": [
    "fpath=r'../project/archive/clubbing/3 (32).PNG'\n",
    "img=plt.imread(fpath)\n",
    "print ('Input image shape is ', img.shape)\n",
    "# resize the image so it is the same size as the images the model was trained on\n",
    "img=cv2.resize(img, img_size) # in earlier code img_size=(224,224) was used for training the model\n",
    "print ('the resized image has shape ', img.shape)\n",
    "### show the resized image\n",
    "plt.imshow(img)\n",
    "# Normally the next line of code rescales the images. However the EfficientNet model expects images in the range 0 to 255\n",
    "# img= img/255\n",
    "# plt.imread returns a numpy array so it is not necessary to convert the image to a numpy array\n",
    "# since we have only one image we have to expand the dimensions of img so it is off the form (1,224,224,3)\n",
    "# where the first dimension 1 is the batch size used by model.predict\n",
    "img=np.expand_dims(img, axis=0)\n",
    "print ('image shape after expanding dimensions is ',img.shape)\n",
    "# now predict the image\n",
    "pred=model.predict(img)\n",
    "print (' the shape of prediction is ', pred.shape)\n",
    "# this dataset has 15 classes so model.predict will return a list of 15 probability values\n",
    "# we want to find the index of the column that has the highest probability\n",
    "index=np.argmax(pred[0])\n",
    "# to get the actual Name of the class earlier Imade a list of the class names called classes\n",
    "klass=classes[index]\n",
    "# lets get the value of the highest probability\n",
    "probability=pred[0][index]*100\n",
    "# print out the class, and the probability \n",
    "print('the image is predicted as being ', klass, ' with a probability of ', probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    !pip install kaggle
Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Requirement already satisfied: kaggle in /usr/local/lib/python3.9/dist-packages (1.5.13)
Requirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from kaggle) (2022.12.7)
Requirement already satisfied: python-slugify in /usr/local/lib/python3.9/dist-packages (from kaggle) (8.0.1)
Requirement already satisfied: urllib3 in /usr/local/lib/python3.9/dist-packages (from kaggle) (1.26.14)
Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from kaggle) (2.25.1)
Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.9/dist-packages (from kaggle) (1.15.0)
Requirement already satisfied: python-dateutil in /usr/local/lib/python3.9/dist-packages (from kaggle) (2.8.2)
Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from kaggle) (4.65.0)
Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.9/dist-packages (from python-slugify->kaggle) (1.3)
Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle) (4.0.0)
Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle) (2.10)
  from google.colab import files
  files.upload()  
!mkdir ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets download -d nuttidalapthanachai/nails-new-test
Downloading nails-new-test.zip to /content
 25% 5.00M/19.7M [00:00<00:00, 35.7MB/s]
100% 19.7M/19.7M [00:00<00:00, 91.4MB/s]
!unzip nails-new-test.zip -d disease
Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Requirement already satisfied: kaggle in /usr/local/lib/python3.9/dist-packages (1.5.13)
Requirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from kaggle) (2022.12.7)
Requirement already satisfied: python-slugify in /usr/local/lib/python3.9/dist-packages (from kaggle) (8.0.1)
Requirement already satisfied: urllib3 in /usr/local/lib/python3.9/dist-packages (from kaggle) (1.26.14)
Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from kaggle) (2.25.1)
Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.9/dist-packages (from kaggle) (1.15.0)
Requirement already satisfied: python-dateutil in /usr/local/lib/python3.9/dist-packages (from kaggle) (2.8.2)
Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from kaggle) (4.65.0)
Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.9/dist-packages (from python-slugify->kaggle) (1.3)
Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle) (4.0.0)
Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle) (2.10)
No file chosen Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable.
Downloading nails-new-test.zip to /content
 25% 5.00M/19.7M [00:00<00:00, 35.7MB/s]
100% 19.7M/19.7M [00:00<00:00, 91.4MB/s]
Output exceeds the size limit. Open the full output data in a text editorArchive:  nails-new-test.zip
  inflating: disease/beau's line/1 (1).jpg  
  inflating: disease/beau's line/1 (10).jpg  
  inflating: disease/beau's line/1 (100).jpg  
  inflating: disease/beau's line/1 (11).jpg  
  inflating: disease/beau's line/1 (12).jpg  
  inflating: disease/beau's line/1 (13).jpg  
  inflating: disease/beau's line/1 (14).jpg  
  inflating: disease/beau's line/1 (15).jpg  
  inflating: disease/beau's line/1 (16).jpg  
  inflating: disease/beau's line/1 (17).jpg  
  inflating: disease/beau's line/1 (18).jpg  
  inflating: disease/beau's line/1 (19).jpg  
  inflating: disease/beau's line/1 (2).jpg  
  inflating: disease/beau's line/1 (20).jpg  
  inflating: disease/beau's line/1 (21).jpg  
  inflating: disease/beau's line/1 (22).jpg  
  inflating: disease/beau's line/1 (23).jpg  
  inflating: disease/beau's line/1 (24).jpg  
  inflating: disease/beau's line/1 (25).jpg  
  inflating: disease/beau's line/1 (26).jpg  
  inflating: disease/beau's line/1 (27).jpg  
  inflating: disease/beau's line/1 (28).jpg  
  inflating: disease/beau's line/1 (29).jpg  
  inflating: disease/beau's line/1 (3).jpg  
...
  inflating: disease/white spot/7 (8).PNG  
  inflating: disease/white spot/7 (8).jpg  
  inflating: disease/white spot/7 (9).PNG  
  inflating: disease/white spot/7 (9).jpg  
Output exceeds the size limit. Open the full output data in a text editorLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Collecting tensorflow==2.9.1
  Downloading tensorflow-2.9.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 511.7/511.7 MB 3.0 MB/s eta 0:00:00
Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (1.22.4)
Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (3.1.0)
Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (3.3.0)
Collecting flatbuffers<2,>=1.12
  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)
Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (23.0)
Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (4.5.0)
Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (3.19.6)
Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (1.51.3)
Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (0.2.0)
Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (0.31.0)
Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (15.0.6.1)
Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (1.4.0)
Collecting tensorboard<2.10,>=2.9
  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.8/5.8 MB 104.7 MB/s eta 0:00:00
Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (0.4.0)
Collecting keras-preprocessing>=1.1.1
  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.6/42.6 KB 5.8 MB/s eta 0:00:00
Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (1.6.3)
...
    Uninstalling tensorflow-2.11.0:
      Successfully uninstalled tensorflow-2.11.0
Successfully installed flatbuffers-1.12 keras-2.9.0 keras-preprocessing-1.1.2 tensorboard-2.9.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0
modules loaded
train_df length:  560   test_df length:  70   valid_df length:  70
[80, 80, 80, 80, 80, 80, 80]
Original Number of classes in dataframe:  7
[80, 80, 80, 80, 80, 80, 80]
Found 80 validated image filenames.
Found 80 validated image filenames.
Found 80 validated image filenames.
Found 80 validated image filenames.
Found 80 validated image filenames.
Found 80 validated image filenames.
Found 80 validated image filenames.
Total Augmented images created=  490
[150, 150, 150, 150, 150, 150, 150]
test batch size:  70   test steps:  1
Found 1050 validated image filenames belonging to 7 classes.
Found 70 validated image filenames belonging to 7 classes.
Found 70 validated image filenames belonging to 7 classes.
Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb3_notop.h5
43941136/43941136 [==============================] - 0s 0us/step
 Epoch     Loss   Accuracy  V_loss    V_acc     LR     Next LR  Monitor  % Improv  Duration
 1 /40     9.846   32.667   9.35768  44.286   0.00100  0.00100  accuracy     0.00    53.01  
 2 /40     8.154   66.000   8.67513  55.714   0.00100  0.00100  accuracy    102.04   20.30  
 3 /40     7.202   83.619   7.74744  65.714   0.00100  0.00100  accuracy    26.70    20.01  
 4 /40     6.663   90.667   7.20667  72.857   0.00100  0.00100  val_loss     6.98    19.99  
 5 /40     6.169   95.143   6.64819  75.714   0.00100  0.00100  val_loss     7.75    20.20  
 6 /40     5.765   96.857   6.16954  77.143   0.00100  0.00100  val_loss     7.20    19.99  
 7 /40     5.419   96.952   5.78750  78.571   0.00100  0.00100  val_loss     6.19    19.97  
 8 /40     5.062   98.190   5.48144  80.000   0.00100  0.00100  val_loss     5.29    19.98  
 9 /40     4.754   98.286   5.11965  80.000   0.00100  0.00100  val_loss     6.60    19.91  
10 /40     4.447   98.857   4.80730  84.286   0.00100  0.00100  val_loss     6.10    20.12  
11 /40     4.159   99.048   4.50447  82.857   0.00100  0.00100  val_loss     6.30    19.82  
12 /40     3.901   99.238   4.24682  85.714   0.00100  0.00100  val_loss     5.72    19.89  
13 /40     3.654   99.429   3.99042  84.286   0.00100  0.00100  val_loss     6.04    20.04  
14 /40     3.403   99.714   3.75506  85.714   0.00100  0.00100  val_loss     5.90    19.86  
15 /40     3.196   99.143   3.51261  85.714   0.00100  0.00100  val_loss     6.46    19.72  
enter H to halt training or an integer for number of epochs to run then ask again
h
training has been halted at epoch 15 due to user input
Training is completed - model is set with weights from epoch 15 
training elapsed time was 0.0 hours, 13.0 minutes, 47.12 seconds)

1/1 [==============================] - 0s 462ms/step - loss: 3.3601 - accuracy: 0.9000
accuracy on the test set is 90.00 %
the save path is:  ./
model was saved as ./EfficientNetB3-nails-89.99.h5
class csv file was saved as ./class_dict.csv
1/1 [==============================] - 0s 207ms/step

Classification Report:
----------------------
                    precision    recall  f1-score   support

      beau's line       1.00      0.60      0.75        10
       black line       0.91      1.00      0.95        10
         clubbing       1.00      1.00      1.00        10
muehrck-e's lines       0.82      0.90      0.86        10
      onycholysis       0.82      0.90      0.86        10
     terry's nail       0.83      1.00      0.91        10
       white spot       1.00      0.90      0.95        10

         accuracy                           0.90        70
        macro avg       0.91      0.90      0.90        70
     weighted avg       0.91      0.90      0.90        70

Input image shape is  (173, 220, 3)
the resized image has shape  (224, 224, 3)
image shape after expanding dimensions is  (1, 224, 224, 3)
1/1 [==============================] - 0s 51ms/step
 the shape of prediction is  (1, 7)
the image is predicted as being  black line  with a probability of  28.738096356391907

Input image shape is  (192, 256, 3)
the resized image has shape  (224, 224, 3)
image shape after expanding dimensions is  (1, 224, 224, 3)
1/1 [==============================] - 0s 55ms/step
 the shape of prediction is  (1, 7)
beau's line
black line
clubbing
muehrck-e's lines
onycholysis
terry's nail
white spot
The image is predicted as being white spot with a probability of 93.36597323417664
#The function then generates a confusion matrix and classification report based on the true labels and predicted classes.

def print_info( test_gen, preds, print_code, save_dir, subject ):
    class_dict=test_gen.class_indices
    labels= test_gen.labels
    file_names= test_gen.filenames 
    error_list=[]
    true_class=[]
    pred_class=[]
    prob_list=[]
    new_dict={}
    error_indices=[]
    y_pred=[]
    for key,value in class_dict.items():
        new_dict[value]=key             # dictionary {integer of class number: string of class name}
    # store new_dict as a text fine in the save_dir
    classes=list(new_dict.values())     # list of string of class names     
    errors=0      
    for i, p in enumerate(preds):
        pred_index=np.argmax(p)         
        true_index=labels[i]  # labels are integer values
        if pred_index != true_index: # a misclassification has occurred
            error_list.append(file_names[i])
            true_class.append(new_dict[true_index])
            pred_class.append(new_dict[pred_index])
            prob_list.append(p[pred_index])
            error_indices.append(true_index)            
            errors=errors + 1
        y_pred.append(pred_index)    
    if print_code !=0:
        if errors>0:
            if print_code>errors:
                r=errors
            else:
                r=print_code           
            msg='{0:^28s}{1:^28s}{2:^28s}{3:^16s}'.format('Filename', 'Predicted Class' , 'True Class', 'Probability')
            print(msg)
            for i in range(r):                
                split1=os.path.split(error_list[i])                
                split2=os.path.split(split1[0])                
                fname=split2[1] + '/' + split1[1]
                msg='{0:^28s}{1:^28s}{2:^28s}{3:4s}{4:^6.4f}'.format(fname, pred_class[i],true_class[i], ' ', prob_list[i])
                print(msg)

                #print(error_list[i]  , pred_class[i], true_class[i], prob_list[i])               
        else:
            msg='With accuracy of 100 % there are no errors to print'
            print(msg)
        if errors>0:
            plot_bar=[]
            plot_class=[]
        for  key, value in new_dict.items():        
            count=error_indices.count(key) 
            if count!=0:
                plot_bar.append(count) # list containg how many times a class c had an error
                plot_class.append(value)   # stores the class 
        fig=plt.figure()
        fig.set_figheight(len(plot_class)/3)
        fig.set_figwidth(10)
        plt.style.use('fivethirtyeight')
        for i in range(0, len(plot_class)):
            c=plot_class[i]
            x=plot_bar[i]
            plt.barh(c, x, )
            plt.title( ' Errors by Class on Test Set')
    y_true= np.array(labels)        
    y_pred=np.array(y_pred)
    if len(classes)<= 30:
        # create a confusion matrix 
        cm = confusion_matrix(y_true, y_pred )        
        length=len(classes)
        if length<8:
            fig_width=8
            fig_height=8
        else:
            fig_width= int(length * .5)
            fig_height= int(length * .5)
        plt.figure(figsize=(fig_width, fig_height))
        sns.heatmap(cm, annot=True, vmin=0, fmt='g', cmap='Blues', cbar=False)       
        plt.xticks(np.arange(length)+.5, classes, rotation= 90)
        plt.yticks(np.arange(length)+.5, classes, rotation=0)
        plt.xlabel("Predicted")
        plt.ylabel("Actual")
        plt.title("Confusion Matrix")
        plt.show()
    clr = classification_report(y_true, y_pred, target_names=classes)
    print("Classification Report:\n----------------------\n", clr)
# this function saving trained model
def saver(save_path, model, model_name, subject, accuracy,img_size, scalar, generator):
    print ('the save path is: ', save_path)
    # first save the model
    save_id=str (model_name +  '-' + subject +'-'+ str(acc)[:str(acc).rfind('.')+3] + '.h5')
    model_save_loc=os.path.join(save_path, save_id)
    model.save(model_save_loc)
    print ('model was saved as ' + model_save_loc) 
    # now create the class_df and convert to csv file    
    class_dict=generator.class_indices 
    height=[]
    width=[]
    scale=[]
    for i in range(len(class_dict)):
        height.append(img_size[0])
        width.append(img_size[1])
        scale.append(scalar)
    Index_series=pd.Series(list(class_dict.values()), name='class_index')
    Class_series=pd.Series(list(class_dict.keys()), name='class') 
    Height_series=pd.Series(height, name='height')
    Width_series=pd.Series(width, name='width')
    Scale_series=pd.Series(scale, name='scale by')
    class_df=pd.concat([Index_series, Class_series, Height_series, Width_series, Scale_series], axis=1)    
    csv_name='class_dict.csv'
    csv_save_loc=os.path.join(save_path, csv_name)
    class_df.to_csv(csv_save_loc, index=False) 
    print('class csv file was saved as ' + csv_save_loc) 

    # Convert SavedModel menjadi satelliteimageclassification.tflite
    
    return model_save_loc, csv_save_loc
#Trim a pandas DataFrame so that each group in a specified column has no more than max_size samples and at least min_size samples.
def trim (df, max_size, min_size, column):
    df=df.copy()
    original_class_count= len(list(df[column].unique()))
    print ('Original Number of classes in dataframe: ', original_class_count)
    sample_list=[] 
    groups=df.groupby(column)
    for label in df[column].unique():        
        group=groups.get_group(label)
        sample_count=len(group)         
        if sample_count> max_size :
            strat=group[column]
            samples,_=train_test_split(group, train_size=max_size, shuffle=True, random_state=123, stratify=strat)            
            sample_list.append(samples)
        elif sample_count>= min_size:
            sample_list.append(group)
    df=pd.concat(sample_list, axis=0).reset_index(drop=True)
    final_class_count= len(list(df[column].unique())) 
    if final_class_count != original_class_count:
        print ('*** WARNING***  dataframe has a reduced number of classes' )
    balance=list(df[column].value_counts())
    print (balance)
    return df
# This function balances the number of samples in each class of a training dataset by creating
def balance(train_df,max_samples, min_samples, column, working_dir, image_size):
    train_df=train_df.copy()
    train_df=trim (train_df, max_samples, min_samples, column)    
    # make directories to store augmented images
    aug_dir=os.path.join(working_dir, 'aug')
    if os.path.isdir(aug_dir):
        shutil.rmtree(aug_dir)
    os.mkdir(aug_dir)
    for label in train_df['labels'].unique():    
        dir_path=os.path.join(aug_dir,label)    
        os.mkdir(dir_path)
    # create and store the augmented images  
    total=0
    gen=ImageDataGenerator(horizontal_flip=True,  rotation_range=20, width_shift_range=.2,
                                  height_shift_range=.2, zoom_range=.2)
    groups=train_df.groupby('labels') # group by class
    for label in train_df['labels'].unique():  # for every class               
        group=groups.get_group(label)  # a dataframe holding only rows with the specified label 
        sample_count=len(group)   # determine how many samples there are in this class  
        if sample_count< max_samples: # if the class has less than target number of images
            aug_img_count=0
            delta=max_samples-sample_count  # number of augmented images to create
            target_dir=os.path.join(aug_dir, label)  # define where to write the images    
            aug_gen=gen.flow_from_dataframe( group,  x_col='filepaths', y_col=None, target_size=image_size,
                                            class_mode=None, batch_size=1, shuffle=False, 
                                            save_to_dir=target_dir, save_prefix='aug-', color_mode='rgb',
                                            save_format='jpg')
            while aug_img_count<delta:
                images=next(aug_gen)            
                aug_img_count += len(images)
            total +=aug_img_count
    print('Total Augmented images created= ', total)
    # create aug_df and merge with train_df to create composite training set ndf
    if total>0:
        aug_fpaths=[]
        aug_labels=[]
        classlist=os.listdir(aug_dir)
        for klass in classlist:
            classpath=os.path.join(aug_dir, klass)     
            flist=os.listdir(classpath)    
            for f in flist:        
                fpath=os.path.join(classpath,f)         
                aug_fpaths.append(fpath)
                aug_labels.append(klass)
        Fseries=pd.Series(aug_fpaths, name='filepaths')
        Lseries=pd.Series(aug_labels, name='labels')
        aug_df=pd.concat([Fseries, Lseries], axis=1)
        train_df=pd.concat([train_df,aug_df], axis=0).reset_index(drop=True)
   
    print (list(train_df['labels'].value_counts()) )
    return train_df 
#  Given a source directory containing subdirectories of images, this function preprocesses the images
#     and returns three dataframes for training, validation, and testing sets.
def preprocess (sdir, trsplit, vsplit):
    filepaths=[]
    labels=[]
    classlist=os.listdir(sdir)
    for klass in classlist:
        classpath=os.path.join(sdir,klass)
        flist=os.listdir(classpath)
        for f in flist:
            fpath=os.path.join(classpath, f)
            filepaths.append(fpath)
            labels.append(klass)
    Fseries=pd.Series(filepaths, name='filepaths')
    Lseries=pd.Series(labels, name='labels')
    df=pd.concat([Fseries, Lseries], axis=1)            
    dsplit=vsplit/(1-trsplit)
    strat=df['labels']
    train_df, dummy_df=train_test_split(df, train_size=trsplit, shuffle=True, random_state=123, stratify=strat)
    strat=dummy_df['labels']
    valid_df, test_df= train_test_split(dummy_df, train_size=dsplit, shuffle=True, random_state=123, stratify=strat)
    print('train_df length: ', len(train_df), '  test_df length: ',len(test_df), '  valid_df length: ', len(valid_df))
     # check that each dataframe has the same number of classes to prevent model.fit errors
    trcount=len(train_df['labels'].unique())
    tecount=len(test_df['labels'].unique())
    vcount=len(valid_df['labels'].unique())
    if trcount < tecount :         
        msg='** WARNING ** number of classes in training set is less than the number of classes in test set'
        print(msg)
        msg='This will throw an error in either model.evaluate or model.predict'
        print(msg)
    if trcount != vcount:
        msg='** WARNING ** number of classes in training set not equal to number of classes in validation set' 
        print(msg)
        msg=' this will throw an error in model.fit'
        print(msg)
        print ('train df class count: ', trcount, 'test df class count: ', tecount, ' valid df class count: ', vcount) 
        ans=input('Enter C to continue execution or H to halt execution')
        if ans =='H' or ans == 'h':
            print('Halting Execution')
            import sys
            sys.exit('program halted by user')            
    print(list(train_df['labels'].value_counts()))
    return train_df, test_df, valid_df
sdir=r'/content/disease'
trsplit=.8
vsplit=.1
train_df, test_df, valid_df= preprocess(sdir,trsplit, vsplit)
max_samples= 150
min_samples=0
column='labels'
working_dir = r'./'
img_size=(224,224)
train_df=balance(train_df, max_samples, min_samples, column, working_dir, img_size)
channels=3
batch_size=40
img_shape=(img_size[0], img_size[1], channels)
length=len(test_df)
test_batch_size=sorted([int(length/n) for n in range(1,length+1) if length % n ==0 and length/n<=80],reverse=True)[0]  
test_steps=int(length/test_batch_size)
print ( 'test batch size: ' ,test_batch_size, '  test steps: ', test_steps)
def scalar(img):    
    return img  # EfficientNet expects pixelsin range 0 to 255 so no scaling is required
trgen=ImageDataGenerator(preprocessing_function=scalar, horizontal_flip=True)
tvgen=ImageDataGenerator(preprocessing_function=scalar)
train_gen=trgen.flow_from_dataframe( train_df, x_col='filepaths', y_col='labels', target_size=img_size, class_mode='categorical',
                                    color_mode='rgb', shuffle=True, batch_size=batch_size)
test_gen=tvgen.flow_from_dataframe( test_df, x_col='filepaths', y_col='labels', target_size=img_size, class_mode='categorical',
                                    color_mode='rgb', shuffle=False, batch_size=test_batch_size)

valid_gen=tvgen.flow_from_dataframe( valid_df, x_col='filepaths', y_col='labels', target_size=img_size, class_mode='categorical',
                                    color_mode='rgb', shuffle=True, batch_size=batch_size)
classes=list(train_gen.class_indices.keys())
class_count=len(classes)
train_steps=int(np.ceil(len(train_gen.labels)/batch_size))
model_name='EfficientNetB3'
base_model=tf.keras.applications.EfficientNetB3(include_top=False, weights="imagenet",input_shape=img_shape, pooling='max') 
x=base_model.output
x=keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(x)
x = Dense(256, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),
                bias_regularizer=regularizers.l1(0.006) ,activation='relu')(x)
x=Dropout(rate=.45, seed=123)(x)        
output=Dense(class_count, activation='softmax')(x)
model=Model(inputs=base_model.input, outputs=output)
model.compile(Adamax(learning_rate=.001), loss='categorical_crossentropy', metrics=['accuracy']) 
epochs =40
patience= 1 # number of epochs to wait to adjust lr if monitored value does not improve
stop_patience =3 # number of epochs to wait before stopping training if monitored value does not improve
threshold=.9 # if train accuracy is < threshhold adjust monitor accuracy, else monitor validation loss
factor=.5 # factor to reduce lr by
dwell=True # experimental, if True and monitored metric does not improve on current epoch set  modelweights back to weights of previous epoch
freeze=False # if true free weights of  the base model
ask_epoch=15# number of epochs to run before asking if you want to halt training
batches=train_steps
callbacks=[LRA(model=model,base_model= base_model,patience=patience,stop_patience=stop_patience, threshold=threshold,
                   factor=factor,dwell=dwell, batches=batches,initial_epoch=0,epochs=epochs, ask_epoch=ask_epoch )]
Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Requirement already satisfied: kaggle in /usr/local/lib/python3.9/dist-packages (1.5.13)
Requirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from kaggle) (2022.12.7)
Requirement already satisfied: python-slugify in /usr/local/lib/python3.9/dist-packages (from kaggle) (8.0.1)
Requirement already satisfied: urllib3 in /usr/local/lib/python3.9/dist-packages (from kaggle) (1.26.14)
Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from kaggle) (2.25.1)
Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.9/dist-packages (from kaggle) (1.15.0)
Requirement already satisfied: python-dateutil in /usr/local/lib/python3.9/dist-packages (from kaggle) (2.8.2)
Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from kaggle) (4.65.0)
Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.9/dist-packages (from python-slugify->kaggle) (1.3)
Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle) (4.0.0)
Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle) (2.10)
No file chosen Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable.
Downloading nails-new-test.zip to /content
 25% 5.00M/19.7M [00:00<00:00, 35.7MB/s]
100% 19.7M/19.7M [00:00<00:00, 91.4MB/s]
Output exceeds the size limit. Open the full output data in a text editorArchive:  nails-new-test.zip
  inflating: disease/beau's line/1 (1).jpg  
  inflating: disease/beau's line/1 (10).jpg  
  inflating: disease/beau's line/1 (100).jpg  
  inflating: disease/beau's line/1 (11).jpg  
  inflating: disease/beau's line/1 (12).jpg  
  inflating: disease/beau's line/1 (13).jpg  
  inflating: disease/beau's line/1 (14).jpg  
  inflating: disease/beau's line/1 (15).jpg  
  inflating: disease/beau's line/1 (16).jpg  
  inflating: disease/beau's line/1 (17).jpg  
  inflating: disease/beau's line/1 (18).jpg  
  inflating: disease/beau's line/1 (19).jpg  
  inflating: disease/beau's line/1 (2).jpg  
  inflating: disease/beau's line/1 (20).jpg  
  inflating: disease/beau's line/1 (21).jpg  
  inflating: disease/beau's line/1 (22).jpg  
  inflating: disease/beau's line/1 (23).jpg  
  inflating: disease/beau's line/1 (24).jpg  
  inflating: disease/beau's line/1 (25).jpg  
  inflating: disease/beau's line/1 (26).jpg  
  inflating: disease/beau's line/1 (27).jpg  
  inflating: disease/beau's line/1 (28).jpg  
  inflating: disease/beau's line/1 (29).jpg  
  inflating: disease/beau's line/1 (3).jpg  
...
  inflating: disease/white spot/7 (8).PNG  
  inflating: disease/white spot/7 (8).jpg  
  inflating: disease/white spot/7 (9).PNG  
  inflating: disease/white spot/7 (9).jpg  
Output exceeds the size limit. Open the full output data in a text editorLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Collecting tensorflow==2.9.1
  Downloading tensorflow-2.9.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 511.7/511.7 MB 3.0 MB/s eta 0:00:00
Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (1.22.4)
Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (3.1.0)
Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (3.3.0)
Collecting flatbuffers<2,>=1.12
  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)
Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (23.0)
Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (4.5.0)
Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (3.19.6)
Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (1.51.3)
Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (0.2.0)
Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (0.31.0)
Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (15.0.6.1)
Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (1.4.0)
Collecting tensorboard<2.10,>=2.9
  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.8/5.8 MB 104.7 MB/s eta 0:00:00
Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (0.4.0)
Collecting keras-preprocessing>=1.1.1
  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.6/42.6 KB 5.8 MB/s eta 0:00:00
Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (1.6.3)
...
    Uninstalling tensorflow-2.11.0:
      Successfully uninstalled tensorflow-2.11.0
Successfully installed flatbuffers-1.12 keras-2.9.0 keras-preprocessing-1.1.2 tensorboard-2.9.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0
modules loaded
train_df length:  560   test_df length:  70   valid_df length:  70
[80, 80, 80, 80, 80, 80, 80]
Original Number of classes in dataframe:  7
[80, 80, 80, 80, 80, 80, 80]
Found 80 validated image filenames.
Found 80 validated image filenames.
Found 80 validated image filenames.
Found 80 validated image filenames.
Found 80 validated image filenames.
Found 80 validated image filenames.
Found 80 validated image filenames.
Total Augmented images created=  490
[150, 150, 150, 150, 150, 150, 150]
test batch size:  70   test steps:  1
Found 1050 validated image filenames belonging to 7 classes.
Found 70 validated image filenames belonging to 7 classes.
Found 70 validated image filenames belonging to 7 classes.
Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb3_notop.h5
43941136/43941136 [==============================] - 0s 0us/step
 Epoch     Loss   Accuracy  V_loss    V_acc     LR     Next LR  Monitor  % Improv  Duration
 1 /40     9.846   32.667   9.35768  44.286   0.00100  0.00100  accuracy     0.00    53.01  
 2 /40     8.154   66.000   8.67513  55.714   0.00100  0.00100  accuracy    102.04   20.30  
 3 /40     7.202   83.619   7.74744  65.714   0.00100  0.00100  accuracy    26.70    20.01  
 4 /40     6.663   90.667   7.20667  72.857   0.00100  0.00100  val_loss     6.98    19.99  
 5 /40     6.169   95.143   6.64819  75.714   0.00100  0.00100  val_loss     7.75    20.20  
 6 /40     5.765   96.857   6.16954  77.143   0.00100  0.00100  val_loss     7.20    19.99  
 7 /40     5.419   96.952   5.78750  78.571   0.00100  0.00100  val_loss     6.19    19.97  
 8 /40     5.062   98.190   5.48144  80.000   0.00100  0.00100  val_loss     5.29    19.98  
 9 /40     4.754   98.286   5.11965  80.000   0.00100  0.00100  val_loss     6.60    19.91  
10 /40     4.447   98.857   4.80730  84.286   0.00100  0.00100  val_loss     6.10    20.12  
11 /40     4.159   99.048   4.50447  82.857   0.00100  0.00100  val_loss     6.30    19.82  
12 /40     3.901   99.238   4.24682  85.714   0.00100  0.00100  val_loss     5.72    19.89  
13 /40     3.654   99.429   3.99042  84.286   0.00100  0.00100  val_loss     6.04    20.04  
14 /40     3.403   99.714   3.75506  85.714   0.00100  0.00100  val_loss     5.90    19.86  
15 /40     3.196   99.143   3.51261  85.714   0.00100  0.00100  val_loss     6.46    19.72  
enter H to halt training or an integer for number of epochs to run then ask again
h
training has been halted at epoch 15 due to user input
Training is completed - model is set with weights from epoch 15 
training elapsed time was 0.0 hours, 13.0 minutes, 47.12 seconds)

1/1 [==============================] - 0s 462ms/step - loss: 3.3601 - accuracy: 0.9000
accuracy on the test set is 90.00 %
the save path is:  ./
model was saved as ./EfficientNetB3-nails-89.99.h5
class csv file was saved as ./class_dict.csv
1/1 [==============================] - 0s 207ms/step

Classification Report:
----------------------
                    precision    recall  f1-score   support

      beau's line       1.00      0.60      0.75        10
       black line       0.91      1.00      0.95        10
         clubbing       1.00      1.00      1.00        10
muehrck-e's lines       0.82      0.90      0.86        10
      onycholysis       0.82      0.90      0.86        10
     terry's nail       0.83      1.00      0.91        10
       white spot       1.00      0.90      0.95        10

         accuracy                           0.90        70
        macro avg       0.91      0.90      0.90        70
     weighted avg       0.91      0.90      0.90        70

Input image shape is  (173, 220, 3)
the resized image has shape  (224, 224, 3)
image shape after expanding dimensions is  (1, 224, 224, 3)
1/1 [==============================] - 0s 51ms/step
 the shape of prediction is  (1, 7)
the image is predicted as being  black line  with a probability of  28.738096356391907

Input image shape is  (192, 256, 3)
the resized image has shape  (224, 224, 3)
image shape after expanding dimensions is  (1, 224, 224, 3)
1/1 [==============================] - 0s 55ms/step
 the shape of prediction is  (1, 7)
beau's line
black line
clubbing
muehrck-e's lines
onycholysis
terry's nail
white spot
The image is predicted as being white spot with a probability of 93.36597323417664
Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Requirement already satisfied: kaggle in /usr/local/lib/python3.9/dist-packages (1.5.13)
Requirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from kaggle) (2022.12.7)
Requirement already satisfied: python-slugify in /usr/local/lib/python3.9/dist-packages (from kaggle) (8.0.1)
Requirement already satisfied: urllib3 in /usr/local/lib/python3.9/dist-packages (from kaggle) (1.26.14)
Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from kaggle) (2.25.1)
Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.9/dist-packages (from kaggle) (1.15.0)
Requirement already satisfied: python-dateutil in /usr/local/lib/python3.9/dist-packages (from kaggle) (2.8.2)
Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from kaggle) (4.65.0)
Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.9/dist-packages (from python-slugify->kaggle) (1.3)
Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle) (4.0.0)
Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle) (2.10)
No file chosen Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable.
Downloading nails-new-test.zip to /content
 25% 5.00M/19.7M [00:00<00:00, 35.7MB/s]
100% 19.7M/19.7M [00:00<00:00, 91.4MB/s]
Output exceeds the size limit. Open the full output data in a text editorArchive:  nails-new-test.zip
  inflating: disease/beau's line/1 (1).jpg  
  inflating: disease/beau's line/1 (10).jpg  
  inflating: disease/beau's line/1 (100).jpg  
  inflating: disease/beau's line/1 (11).jpg  
  inflating: disease/beau's line/1 (12).jpg  
  inflating: disease/beau's line/1 (13).jpg  
  inflating: disease/beau's line/1 (14).jpg  
  inflating: disease/beau's line/1 (15).jpg  
  inflating: disease/beau's line/1 (16).jpg  
  inflating: disease/beau's line/1 (17).jpg  
  inflating: disease/beau's line/1 (18).jpg  
  inflating: disease/beau's line/1 (19).jpg  
  inflating: disease/beau's line/1 (2).jpg  
  inflating: disease/beau's line/1 (20).jpg  
  inflating: disease/beau's line/1 (21).jpg  
  inflating: disease/beau's line/1 (22).jpg  
  inflating: disease/beau's line/1 (23).jpg  
  inflating: disease/beau's line/1 (24).jpg  
  inflating: disease/beau's line/1 (25).jpg  
  inflating: disease/beau's line/1 (26).jpg  
  inflating: disease/beau's line/1 (27).jpg  
  inflating: disease/beau's line/1 (28).jpg  
  inflating: disease/beau's line/1 (29).jpg  
  inflating: disease/beau's line/1 (3).jpg  
...
  inflating: disease/white spot/7 (8).PNG  
  inflating: disease/white spot/7 (8).jpg  
  inflating: disease/white spot/7 (9).PNG  
  inflating: disease/white spot/7 (9).jpg  
Output exceeds the size limit. Open the full output data in a text editorLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Collecting tensorflow==2.9.1
  Downloading tensorflow-2.9.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 511.7/511.7 MB 3.0 MB/s eta 0:00:00
Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (1.22.4)
Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (3.1.0)
Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (3.3.0)
Collecting flatbuffers<2,>=1.12
  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)
Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (23.0)
Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (4.5.0)
Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (3.19.6)
Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (1.51.3)
Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (0.2.0)
Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (0.31.0)
Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (15.0.6.1)
Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (1.4.0)
Collecting tensorboard<2.10,>=2.9
  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.8/5.8 MB 104.7 MB/s eta 0:00:00
Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (0.4.0)
Collecting keras-preprocessing>=1.1.1
  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.6/42.6 KB 5.8 MB/s eta 0:00:00
Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (1.6.3)
...
    Uninstalling tensorflow-2.11.0:
      Successfully uninstalled tensorflow-2.11.0
Successfully installed flatbuffers-1.12 keras-2.9.0 keras-preprocessing-1.1.2 tensorboard-2.9.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0
modules loaded
train_df length:  560   test_df length:  70   valid_df length:  70
[80, 80, 80, 80, 80, 80, 80]
Original Number of classes in dataframe:  7
[80, 80, 80, 80, 80, 80, 80]
Found 80 validated image filenames.
Found 80 validated image filenames.
Found 80 validated image filenames.
Found 80 validated image filenames.
Found 80 validated image filenames.
Found 80 validated image filenames.
Found 80 validated image filenames.
Total Augmented images created=  490
[150, 150, 150, 150, 150, 150, 150]
test batch size:  70   test steps:  1
Found 1050 validated image filenames belonging to 7 classes.
Found 70 validated image filenames belonging to 7 classes.
Found 70 validated image filenames belonging to 7 classes.
Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb3_notop.h5
43941136/43941136 [==============================] - 0s 0us/step
 Epoch     Loss   Accuracy  V_loss    V_acc     LR     Next LR  Monitor  % Improv  Duration
 1 /40     9.846   32.667   9.35768  44.286   0.00100  0.00100  accuracy     0.00    53.01  
 2 /40     8.154   66.000   8.67513  55.714   0.00100  0.00100  accuracy    102.04   20.30  
 3 /40     7.202   83.619   7.74744  65.714   0.00100  0.00100  accuracy    26.70    20.01  
 4 /40     6.663   90.667   7.20667  72.857   0.00100  0.00100  val_loss     6.98    19.99  
 5 /40     6.169   95.143   6.64819  75.714   0.00100  0.00100  val_loss     7.75    20.20  
 6 /40     5.765   96.857   6.16954  77.143   0.00100  0.00100  val_loss     7.20    19.99  
 7 /40     5.419   96.952   5.78750  78.571   0.00100  0.00100  val_loss     6.19    19.97  
 8 /40     5.062   98.190   5.48144  80.000   0.00100  0.00100  val_loss     5.29    19.98  
 9 /40     4.754   98.286   5.11965  80.000   0.00100  0.00100  val_loss     6.60    19.91  
10 /40     4.447   98.857   4.80730  84.286   0.00100  0.00100  val_loss     6.10    20.12  
11 /40     4.159   99.048   4.50447  82.857   0.00100  0.00100  val_loss     6.30    19.82  
12 /40     3.901   99.238   4.24682  85.714   0.00100  0.00100  val_loss     5.72    19.89  
13 /40     3.654   99.429   3.99042  84.286   0.00100  0.00100  val_loss     6.04    20.04  
14 /40     3.403   99.714   3.75506  85.714   0.00100  0.00100  val_loss     5.90    19.86  
15 /40     3.196   99.143   3.51261  85.714   0.00100  0.00100  val_loss     6.46    19.72  
enter H to halt training or an integer for number of epochs to run then ask again
h
training has been halted at epoch 15 due to user input
Training is completed - model is set with weights from epoch 15 
training elapsed time was 0.0 hours, 13.0 minutes, 47.12 seconds)

1/1 [==============================] - 0s 462ms/step - loss: 3.3601 - accuracy: 0.9000
accuracy on the test set is 90.00 %
the save path is:  ./
model was saved as ./EfficientNetB3-nails-89.99.h5
class csv file was saved as ./class_dict.csv
1/1 [==============================] - 0s 207ms/step

Classification Report:
----------------------
                    precision    recall  f1-score   support

      beau's line       1.00      0.60      0.75        10
       black line       0.91      1.00      0.95        10
         clubbing       1.00      1.00      1.00        10
muehrck-e's lines       0.82      0.90      0.86        10
      onycholysis       0.82      0.90      0.86        10
     terry's nail       0.83      1.00      0.91        10
       white spot       1.00      0.90      0.95        10

         accuracy                           0.90        70
        macro avg       0.91      0.90      0.90        70
     weighted avg       0.91      0.90      0.90        70

Input image shape is  (173, 220, 3)
the resized image has shape  (224, 224, 3)
image shape after expanding dimensions is  (1, 224, 224, 3)
1/1 [==============================] - 0s 51ms/step
 the shape of prediction is  (1, 7)
the image is predicted as being  black line  with a probability of  28.738096356391907

Input image shape is  (192, 256, 3)
the resized image has shape  (224, 224, 3)
image shape after expanding dimensions is  (1, 224, 224, 3)
1/1 [==============================] - 0s 55ms/step
 the shape of prediction is  (1, 7)
beau's line
black line
clubbing
muehrck-e's lines
onycholysis
terry's nail
white spot
The image is predicted as being white spot with a probability of 93.36597323417664
fpath=r'/content/drive/MyDrive/gayantha/220px-White_spot_in_nail.jpg'
img=plt.imread(fpath)
print ('Input image shape is ', img.shape)
# resize the image so it is the same size as the images the model was trained on
img=cv2.resize(img, img_size) # in earlier code img_size=(224,224) was used for training the model
print ('the resized image has shape ', img.shape)
### show the resized image
plt.imshow(img)
# Normally the next line of code rescales the images. However the EfficientNet model expects images in the range 0 to 255
# img= img/255
# plt.imread returns a numpy array so it is not necessary to convert the image to a numpy array
# since we have only one image we have to expand the dimensions of img so it is off the form (1,224,224,3)
# where the first dimension 1 is the batch size used by model.predict
img=np.expand_dims(img, axis=0)
print ('image shape after expanding dimensions is ',img.shape)
# now predict the image
pred=model.predict(img)
print (' the shape of prediction is ', pred.shape)
# we want to find the index of the column that has the highest probability
index=np.argmax(pred[0])
# to get the actual Name of the class earlier Imade a list of the class names called classes
klass=classes[index]
# lets get the value of the highest probability
probability=pred[0][index]*100
# print out the class, and the probability 
print('the image is predicted as being ', klass, ' with a probability of ', probability)
Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Requirement already satisfied: kaggle in /usr/local/lib/python3.9/dist-packages (1.5.13)
Requirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from kaggle) (2022.12.7)
Requirement already satisfied: python-slugify in /usr/local/lib/python3.9/dist-packages (from kaggle) (8.0.1)
Requirement already satisfied: urllib3 in /usr/local/lib/python3.9/dist-packages (from kaggle) (1.26.14)
Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from kaggle) (2.25.1)
Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.9/dist-packages (from kaggle) (1.15.0)
Requirement already satisfied: python-dateutil in /usr/local/lib/python3.9/dist-packages (from kaggle) (2.8.2)
Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from kaggle) (4.65.0)
Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.9/dist-packages (from python-slugify->kaggle) (1.3)
Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle) (4.0.0)
Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle) (2.10)
No file chosen Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable.
Downloading nails-new-test.zip to /content
 25% 5.00M/19.7M [00:00<00:00, 35.7MB/s]
100% 19.7M/19.7M [00:00<00:00, 91.4MB/s]
Output exceeds the size limit. Open the full output data in a text editorArchive:  nails-new-test.zip
  inflating: disease/beau's line/1 (1).jpg  
  inflating: disease/beau's line/1 (10).jpg  
  inflating: disease/beau's line/1 (100).jpg  
  inflating: disease/beau's line/1 (11).jpg  
  inflating: disease/beau's line/1 (12).jpg  
  inflating: disease/beau's line/1 (13).jpg  
  inflating: disease/beau's line/1 (14).jpg  
  inflating: disease/beau's line/1 (15).jpg  
  inflating: disease/beau's line/1 (16).jpg  
  inflating: disease/beau's line/1 (17).jpg  
  inflating: disease/beau's line/1 (18).jpg  
  inflating: disease/beau's line/1 (19).jpg  
  inflating: disease/beau's line/1 (2).jpg  
  inflating: disease/beau's line/1 (20).jpg  
  inflating: disease/beau's line/1 (21).jpg  
  inflating: disease/beau's line/1 (22).jpg  
  inflating: disease/beau's line/1 (23).jpg  
  inflating: disease/beau's line/1 (24).jpg  
  inflating: disease/beau's line/1 (25).jpg  
  inflating: disease/beau's line/1 (26).jpg  
  inflating: disease/beau's line/1 (27).jpg  
  inflating: disease/beau's line/1 (28).jpg  
  inflating: disease/beau's line/1 (29).jpg  
  inflating: disease/beau's line/1 (3).jpg  
...
  inflating: disease/white spot/7 (8).PNG  
  inflating: disease/white spot/7 (8).jpg  
  inflating: disease/white spot/7 (9).PNG  
  inflating: disease/white spot/7 (9).jpg  
Output exceeds the size limit. Open the full output data in a text editorLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Collecting tensorflow==2.9.1
  Downloading tensorflow-2.9.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 511.7/511.7 MB 3.0 MB/s eta 0:00:00
Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (1.22.4)
Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (3.1.0)
Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (3.3.0)
Collecting flatbuffers<2,>=1.12
  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)
Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (23.0)
Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (4.5.0)
Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (3.19.6)
Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (1.51.3)
Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (0.2.0)
Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (0.31.0)
Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (15.0.6.1)
Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (1.4.0)
Collecting tensorboard<2.10,>=2.9
  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.8/5.8 MB 104.7 MB/s eta 0:00:00
Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (0.4.0)
Collecting keras-preprocessing>=1.1.1
  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.6/42.6 KB 5.8 MB/s eta 0:00:00
Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.1) (1.6.3)
...
    Uninstalling tensorflow-2.11.0:
      Successfully uninstalled tensorflow-2.11.0
Successfully installed flatbuffers-1.12 keras-2.9.0 keras-preprocessing-1.1.2 tensorboard-2.9.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0
modules loaded
train_df length:  560   test_df length:  70   valid_df length:  70
[80, 80, 80, 80, 80, 80, 80]
Original Number of classes in dataframe:  7
[80, 80, 80, 80, 80, 80, 80]
Found 80 validated image filenames.
Found 80 validated image filenames.
Found 80 validated image filenames.
Found 80 validated image filenames.
Found 80 validated image filenames.
Found 80 validated image filenames.
Found 80 validated image filenames.
Total Augmented images created=  490
[150, 150, 150, 150, 150, 150, 150]
test batch size:  70   test steps:  1
Found 1050 validated image filenames belonging to 7 classes.
Found 70 validated image filenames belonging to 7 classes.
Found 70 validated image filenames belonging to 7 classes.
Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb3_notop.h5
43941136/43941136 [==============================] - 0s 0us/step
 Epoch     Loss   Accuracy  V_loss    V_acc     LR     Next LR  Monitor  % Improv  Duration
 1 /40     9.846   32.667   9.35768  44.286   0.00100  0.00100  accuracy     0.00    53.01  
 2 /40     8.154   66.000   8.67513  55.714   0.00100  0.00100  accuracy    102.04   20.30  
 3 /40     7.202   83.619   7.74744  65.714   0.00100  0.00100  accuracy    26.70    20.01  
 4 /40     6.663   90.667   7.20667  72.857   0.00100  0.00100  val_loss     6.98    19.99  
 5 /40     6.169   95.143   6.64819  75.714   0.00100  0.00100  val_loss     7.75    20.20  
 6 /40     5.765   96.857   6.16954  77.143   0.00100  0.00100  val_loss     7.20    19.99  
 7 /40     5.419   96.952   5.78750  78.571   0.00100  0.00100  val_loss     6.19    19.97  
 8 /40     5.062   98.190   5.48144  80.000   0.00100  0.00100  val_loss     5.29    19.98  
 9 /40     4.754   98.286   5.11965  80.000   0.00100  0.00100  val_loss     6.60    19.91  
10 /40     4.447   98.857   4.80730  84.286   0.00100  0.00100  val_loss     6.10    20.12  
11 /40     4.159   99.048   4.50447  82.857   0.00100  0.00100  val_loss     6.30    19.82  
12 /40     3.901   99.238   4.24682  85.714   0.00100  0.00100  val_loss     5.72    19.89  
13 /40     3.654   99.429   3.99042  84.286   0.00100  0.00100  val_loss     6.04    20.04  
14 /40     3.403   99.714   3.75506  85.714   0.00100  0.00100  val_loss     5.90    19.86  
15 /40     3.196   99.143   3.51261  85.714   0.00100  0.00100  val_loss     6.46    19.72  
enter H to halt training or an integer for number of epochs to run then ask again
h
training has been halted at epoch 15 due to user input
Training is completed - model is set with weights from epoch 15 
training elapsed time was 0.0 hours, 13.0 minutes, 47.12 seconds)

1/1 [==============================] - 0s 462ms/step - loss: 3.3601 - accuracy: 0.9000
accuracy on the test set is 90.00 %
the save path is:  ./
model was saved as ./EfficientNetB3-nails-89.99.h5
class csv file was saved as ./class_dict.csv
1/1 [==============================] - 0s 207ms/step

Classification Report:
----------------------
                    precision    recall  f1-score   support

      beau's line       1.00      0.60      0.75        10
       black line       0.91      1.00      0.95        10
         clubbing       1.00      1.00      1.00        10
muehrck-e's lines       0.82      0.90      0.86        10
      onycholysis       0.82      0.90      0.86        10
     terry's nail       0.83      1.00      0.91        10
       white spot       1.00      0.90      0.95        10

         accuracy                           0.90        70
        macro avg       0.91      0.90      0.90        70
     weighted avg       0.91      0.90      0.90        70

Input image shape is  (173, 220, 3)
the resized image has shape  (224, 224, 3)
image shape after expanding dimensions is  (1, 224, 224, 3)
1/1 [==============================] - 0s 51ms/step
 the shape of prediction is  (1, 7)
the image is predicted as being  black line  with a probability of  28.738096356391907

Input image shape is  (192, 256, 3)
the resized image has shape  (224, 224, 3)
image shape after expanding dimensions is  (1, 224, 224, 3)
1/1 [==============================] - 0s 55ms/step
 the shape of prediction is  (1, 7)
beau's line
black line
clubbing
muehrck-e's lines
onycholysis
terry's nail
white spot
The image is predicted as being white spot with a probability of 93.36597323417664
# load model and and use it to predict the class of an image
# if this function use seperate using treaing model 
# when this function use seperate must import tensorflow as tf / import numpy as np  / import cv2
def predict(img):
    model_path = 'EfficientNetB3-nails-89.99.h5'
    model = tf.keras.models.load_model(model_path)
    fpath=img
    img=plt.imread(fpath)
    img_size = (224,224);
    fpath=img
    img=plt.imread(fpath)
    # Resize the image
    resized_image = cv2.resize(img, img_size)
    # img=cv2.resize(image,img_size);
    img=np.expand_dims(resized_image, axis=0);
    pred=model.predict(img);
    index=np.argmax(pred[0]);
    classes =["beau's line","black line","clubbing","muehrck-e's lines","onycholysis","terry's nail","white spot"];
    klass=classes[index];
    probability = pred[0][index] * 100;
    if probability < 30:
        print("Try again.")
        return "Try again."
    else:
        # Print out the class and the probability.
        print('The image is predicted as being', klass, 'with a probability of', probability)
        return klass

